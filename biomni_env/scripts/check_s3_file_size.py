#!/usr/bin/env python3
"""
S3æ–‡ä»¶å¤§å°æ£€æŸ¥å·¥å…·
ç›´æ¥æŸ¥è¯¢S3å­˜å‚¨æ¡¶ä¸­ç‰¹å®šæ–‡ä»¶çš„å¤§å°å’Œå¯è®¿é—®æ€§
"""

import requests
import os
import sys
from urllib.parse import urljoin
from typing import Dict, List, Tuple, Optional
import time

def check_file_size(s3_bucket_url: str, file_path: str) -> Dict[str, any]:
    """
    æ£€æŸ¥S3å­˜å‚¨æ¡¶ä¸­ç‰¹å®šæ–‡ä»¶çš„å¤§å°å’Œå¯è®¿é—®æ€§
    
    Args:
        s3_bucket_url: S3å­˜å‚¨æ¡¶çš„åŸºç¡€URL (e.g., "https://biomni-release.s3.amazonaws.com")
        file_path: æ–‡ä»¶åœ¨å­˜å‚¨æ¡¶ä¸­çš„è·¯å¾„ (e.g., "data_lake/gene_info.parquet")
        
    Returns:
        åŒ…å«æ–‡ä»¶ä¿¡æ¯çš„å­—å…¸
    """
    
    # æ„å»ºå®Œæ•´çš„æ–‡ä»¶URL
    file_url = urljoin(s3_bucket_url + "/", file_path)
    
    print(f"ğŸ” æ£€æŸ¥æ–‡ä»¶: {file_path}")
    print(f"ğŸ“¡ URL: {file_url}")
    
    try:
        # å‘é€HEADè¯·æ±‚æ¥è·å–æ–‡ä»¶ä¿¡æ¯ï¼ˆä¸ä¸‹è½½å†…å®¹ï¼‰
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        response = requests.head(file_url, headers=headers, timeout=30)
        
        result = {
            "file_path": file_path,
            "url": file_url,
            "status_code": response.status_code,
            "accessible": False,
            "size_bytes": 0,
            "size_mb": 0,
            "content_type": "",
            "last_modified": "",
            "etag": "",
            "error": None
        }
        
        if response.status_code == 200:
            # æ–‡ä»¶å­˜åœ¨ä¸”å¯è®¿é—®
            result["accessible"] = True
            
            # è·å–æ–‡ä»¶å¤§å°
            content_length = response.headers.get('content-length')
            if content_length:
                result["size_bytes"] = int(content_length)
                result["size_mb"] = round(int(content_length) / (1024 * 1024), 2)
            
            # è·å–å…¶ä»–ä¿¡æ¯
            result["content_type"] = response.headers.get('content-type', '')
            result["last_modified"] = response.headers.get('last-modified', '')
            result["etag"] = response.headers.get('etag', '')
            
            print(f"âœ… æ–‡ä»¶å¯è®¿é—®")
            print(f"   å¤§å°: {format_file_size(result['size_bytes'])}")
            print(f"   ç±»å‹: {result['content_type']}")
            if result['last_modified']:
                print(f"   ä¿®æ”¹æ—¶é—´: {result['last_modified']}")
                
        elif response.status_code == 404:
            result["error"] = "æ–‡ä»¶ä¸å­˜åœ¨"
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨ (404)")
        elif response.status_code == 403:
            result["error"] = "è®¿é—®è¢«æ‹’ç»"
            print(f"âŒ è®¿é—®è¢«æ‹’ç» (403)")
        else:
            result["error"] = f"HTTPé”™è¯¯: {response.status_code}"
            print(f"âŒ HTTPé”™è¯¯: {response.status_code}")
            
        return result
        
    except requests.exceptions.Timeout:
        error_msg = "è¯·æ±‚è¶…æ—¶"
        print(f"âŒ {error_msg}")
        return {
            "file_path": file_path,
            "url": file_url,
            "status_code": None,
            "accessible": False,
            "size_bytes": 0,
            "size_mb": 0,
            "error": error_msg
        }
    except requests.exceptions.RequestException as e:
        error_msg = f"è¯·æ±‚å¼‚å¸¸: {e}"
        print(f"âŒ {error_msg}")
        return {
            "file_path": file_path,
            "url": file_url,
            "status_code": None,
            "accessible": False,
            "size_bytes": 0,
            "size_mb": 0,
            "error": error_msg
        }
    except Exception as e:
        error_msg = f"æœªçŸ¥é”™è¯¯: {e}"
        print(f"âŒ {error_msg}")
        return {
            "file_path": file_path,
            "url": file_url,
            "status_code": None,
            "accessible": False,
            "size_bytes": 0,
            "size_mb": 0,
            "error": error_msg
        }

def format_file_size(size_bytes: int) -> str:
    """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°æ˜¾ç¤º"""
    if size_bytes == 0:
        return "0 B"
    elif size_bytes < 1024:
        return f"{size_bytes} B"
    elif size_bytes < 1024 * 1024:
        return f"{size_bytes / 1024:.1f} KB"
    elif size_bytes < 1024 * 1024 * 1024:
        return f"{size_bytes / (1024 * 1024):.1f} MB"
    else:
        return f"{size_bytes / (1024 * 1024 * 1024):.1f} GB"

def check_multiple_files(s3_bucket_url: str, file_list: List[str], delay: float = 0.1) -> List[Dict[str, any]]:
    """
    æ£€æŸ¥å¤šä¸ªæ–‡ä»¶çš„å¤§å°å’Œå¯è®¿é—®æ€§
    
    Args:
        s3_bucket_url: S3å­˜å‚¨æ¡¶çš„åŸºç¡€URL
        file_list: æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        delay: è¯·æ±‚ä¹‹é—´çš„å»¶è¿Ÿï¼ˆç§’ï¼‰
        
    Returns:
        æ–‡ä»¶ä¿¡æ¯åˆ—è¡¨
    """
    
    print(f"ğŸš€ å¼€å§‹æ£€æŸ¥ {len(file_list)} ä¸ªæ–‡ä»¶...")
    print("=" * 80)
    
    results = []
    accessible_count = 0
    total_size = 0
    
    for i, file_path in enumerate(file_list, 1):
        print(f"\n[{i}/{len(file_list)}] ", end="")
        result = check_file_size(s3_bucket_url, file_path)
        results.append(result)
        
        if result["accessible"]:
            accessible_count += 1
            total_size += result["size_bytes"]
        
        # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
        if i < len(file_list):
            time.sleep(delay)
    
    # æ‰“å°æ±‡æ€»ä¿¡æ¯
    print(f"\n{'='*20} æ£€æŸ¥ç»“æœæ±‡æ€» {'='*20}")
    print(f"æ€»æ–‡ä»¶æ•°: {len(file_list)}")
    print(f"å¯è®¿é—®æ–‡ä»¶: {accessible_count}")
    print(f"ä¸å¯è®¿é—®æ–‡ä»¶: {len(file_list) - accessible_count}")
    print(f"æˆåŠŸç‡: {accessible_count/len(file_list)*100:.1f}%")
    print(f"æ€»å¤§å°: {format_file_size(total_size)}")
    
    return results

def get_expected_files_from_env_desc() -> List[str]:
    """ä»env_desc.pyè·å–æœŸæœ›çš„æ–‡ä»¶åˆ—è¡¨"""
    try:
        # å°è¯•å¯¼å…¥data_lake_dict
        import sys
        import os
        
        # æ·»åŠ é¡¹ç›®è·¯å¾„
        project_root = os.path.dirname(os.path.abspath(__file__))
        if project_root.endswith('scripts'):
            project_root = os.path.dirname(project_root)
        sys.path.insert(0, project_root)
        
        from biomni.env_desc import data_lake_dict
        
        # è·å–data_lakeæ–‡ä»¶åˆ—è¡¨
        data_lake_files = [f"data_lake/{filename}" for filename in data_lake_dict.keys()]
        
        # æ·»åŠ benchmarkæ–‡ä»¶
        benchmark_files = [
            "benchmark.zip"  # benchmarkæ–‡ä»¶å¤¹é€šå¸¸ä½œä¸ºzipæ–‡ä»¶ä¸‹è½½
        ]
        
        return data_lake_files + benchmark_files
        
    except ImportError as e:
        print(f"âš ï¸ æ— æ³•å¯¼å…¥biomniæ¨¡å—: {e}")
        print("å°†ä½¿ç”¨é»˜è®¤æ–‡ä»¶åˆ—è¡¨")
        return []

def main():
    """ä¸»å‡½æ•°"""
    s3_bucket_url = "https://biomni-release.s3.amazonaws.com"
    
    print("ğŸ§¬ Biomni S3æ–‡ä»¶å¤§å°æ£€æŸ¥å·¥å…·")
    print("=" * 80)
    print(f"ç›®æ ‡å­˜å‚¨æ¡¶: {s3_bucket_url}")
    print("=" * 80)
    
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        # æ£€æŸ¥æŒ‡å®šçš„æ–‡ä»¶
        file_path = sys.argv[1]
        print(f"æ£€æŸ¥æŒ‡å®šæ–‡ä»¶: {file_path}")
        result = check_file_size(s3_bucket_url, file_path)
        
        if result["accessible"]:
            print(f"\nâœ… æ–‡ä»¶ä¿¡æ¯:")
            print(f"   è·¯å¾„: {result['file_path']}")
            print(f"   å¤§å°: {format_file_size(result['size_bytes'])}")
            print(f"   ç±»å‹: {result['content_type']}")
            if result['last_modified']:
                print(f"   ä¿®æ”¹æ—¶é—´: {result['last_modified']}")
        else:
            print(f"\nâŒ æ–‡ä»¶ä¸å¯è®¿é—®: {result['error']}")
    else:
        # æ£€æŸ¥æ‰€æœ‰æœŸæœ›çš„æ–‡ä»¶
        print("è·å–æœŸæœ›çš„æ–‡ä»¶åˆ—è¡¨...")
        expected_files = get_expected_files_from_env_desc()
        
        if not expected_files:
            # ä½¿ç”¨ä¸€äº›å¸¸è§çš„æ–‡ä»¶ä½œä¸ºç¤ºä¾‹
            expected_files = [
                "data_lake/gene_info.parquet",
                "data_lake/hp.obo",
                "data_lake/txgnn_name_mapping.pkl",
                "data_lake/txgnn_prediction.pkl",
                "data_lake/sgRNA/KO_SP_human.txt",
                "data_lake/sgRNA/KO_SP_mouse.txt",
                "benchmark.zip"
            ]
            print("ä½¿ç”¨ç¤ºä¾‹æ–‡ä»¶åˆ—è¡¨")
        
        print(f"å°†æ£€æŸ¥ {len(expected_files)} ä¸ªæ–‡ä»¶")
        
        # è¯¢é—®ç”¨æˆ·æ˜¯å¦ç»§ç»­
        response = input("\næ˜¯å¦ç»§ç»­æ£€æŸ¥æ‰€æœ‰æ–‡ä»¶? (y/n): ").lower().strip()
        if response in ['y', 'yes', 'æ˜¯']:
            results = check_multiple_files(s3_bucket_url, expected_files)
            
            # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
            output_file = "s3_file_check_results.txt"
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write("S3æ–‡ä»¶æ£€æŸ¥ç»“æœ\n")
                f.write("=" * 50 + "\n")
                f.write(f"æ£€æŸ¥æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"å­˜å‚¨æ¡¶: {s3_bucket_url}\n\n")
                
                for result in results:
                    f.write(f"æ–‡ä»¶: {result['file_path']}\n")
                    f.write(f"çŠ¶æ€: {'å¯è®¿é—®' if result['accessible'] else 'ä¸å¯è®¿é—®'}\n")
                    if result['accessible']:
                        f.write(f"å¤§å°: {format_file_size(result['size_bytes'])}\n")
                        f.write(f"ç±»å‹: {result['content_type']}\n")
                    else:
                        f.write(f"é”™è¯¯: {result['error']}\n")
                    f.write("-" * 30 + "\n")
            
            print(f"\nç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        else:
            print("æ£€æŸ¥å·²å–æ¶ˆ")

if __name__ == "__main__":
    main() 